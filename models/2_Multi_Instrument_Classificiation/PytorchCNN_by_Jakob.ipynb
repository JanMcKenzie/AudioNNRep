{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import fft\n",
    "import wave\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import callbacks\n",
    "from scipy.io.wavfile import write\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.io import wavfile\n",
    "import scipy.signal as signal\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Admin/OneDrive/Skrivebord/Machine Learning/AppliedML2024-main/AppliedML2024/Assignments/Group Project/Shared code/audio/'\n",
    "\n",
    "sys.path.append(\"C:/Users/Admin/OneDrive/Skrivebord/Machine Learning/AppliedML2024-main/AppliedML2024/Assignments/Group Project/Shared code/AudioNNRep/functions\")\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = f.read_files_in_dir(path)\n",
    "organ = [filename for filename in filenames if \"organ\" in filename] \n",
    "bass = [filename for filename in filenames if \"bass\" in filename]\n",
    "guitar = [filename for filename in filenames if \"guitar\" in filename]\n",
    "vocal = [filename for filename in filenames if \"vocal\" in filename] \n",
    "flutes = [filename for filename in filenames if \"flute\" in filename]\n",
    "keyboards = [filename for filename in filenames if \"keyboard\" in filename] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = [organ, bass, guitar, vocal, flutes, keyboards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using f.audio_to_waveform(path + instrument) and f.waveform_to_spectogram(waveform_test) generate mixed_spectogram of three of the instruments: pianos, bass, guitar, drum, flutes, keyboards\n",
    "\n",
    "def generate_mixed_spectrograms(n_mixed_spectrograms, number_of_instruments = 3):\n",
    "    instruments = [organ, bass, guitar, vocal, flutes, keyboards]\n",
    "    picked_inst_arr = np.zeros((n_mixed_spectrograms, len(instruments)))\n",
    "    for i in range(n_mixed_spectrograms):\n",
    "        # for each row, turn 3 random zeros to 1\n",
    "        picked_inst = random.sample(range(len(instruments)), number_of_instruments)\n",
    "        picked_inst_arr[i, picked_inst] = 1\n",
    "    \n",
    "    mixed_spectograms = []\n",
    "    indvidual_spectograms = []\n",
    "    for i in range(n_mixed_spectrograms):\n",
    "        selected_files = []\n",
    "        # Select files from the picked instruments\n",
    "        for j in range(len(instruments)):\n",
    "            if picked_inst_arr[i, j] == 1:\n",
    "                selected_files.append(random.choice(instruments[j]))\n",
    "\n",
    "        # Generate the mixed spectrogram, and save the individual spectrograms\n",
    "        spectogram_i = []\n",
    "        for j in range(len(selected_files)):\n",
    "            waveform_test, sr = f.audio_to_waveform(path + selected_files[j])\n",
    "            spectogram = f.waveform_to_spectogram(waveform_test)\n",
    "            spectogram_i.append(spectogram)\n",
    "            if j == 0:\n",
    "                combined_waveform = waveform_test\n",
    "            else:\n",
    "                combined_waveform = combined_waveform + waveform_test\n",
    "\n",
    "        indvidual_spectograms.append(spectogram_i)\n",
    "\n",
    "        mixed_spectogram = f.waveform_to_spectogram(combined_waveform)\n",
    "        mixed_spectograms.append(mixed_spectogram)\n",
    "\n",
    "    return np.array(mixed_spectograms), indvidual_spectograms, picked_inst_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_spectograms, indvidual_spectograms, picked_inst_arr = generate_mixed_spectrograms(1000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1025, 126)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(mixed_spectograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Activation, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(mixed_spectograms, picked_inst_arr, test_size=0.2, random_state=42)\n",
    "\n",
    "# normalize the data\n",
    "X_train = X_train / np.max(X_train)\n",
    "X_test  = X_test  / np.max(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.AudioDataset object at 0x000001E148477F20>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(X_train, y_train)\n",
    "test_dataset = AudioDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#import device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n",
      "final shape for the model: torch.Size([1, 32, 1025, 126])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_loader:\n",
    "    X, y = X.to(device, dtype=torch.float32), y.to(device, dtype=torch.float32)\n",
    "    X    = torch.unsqueeze(X, 0)\n",
    "    print(\"final shape for the model:\", np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_function(output, target):\n",
    "    return torch.sum(torch.abs(target - output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0765, 0.0000, 0.0400, 0.0380, 0.0185],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 0., 1., 0., 0., 1.])\n",
      "Batch loss: 3.0143\n",
      "tensor([0.0000, 0.0732, 0.0000, 0.0372, 0.0362, 0.0250],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 0., 1., 0., 1., 0.])\n",
      "Batch loss: 3.0001\n",
      "tensor([0.0000, 0.0718, 0.0000, 0.0367, 0.0358, 0.0258],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 0., 1., 1., 0., 0.])\n",
      "Batch loss: 3.0060\n",
      "tensor([0.0000, 0.0699, 0.0000, 0.0367, 0.0365, 0.0287],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 0., 0., 1., 1.])\n",
      "Batch loss: 3.0079\n",
      "tensor([0.0000, 0.0684, 0.0000, 0.0373, 0.0366, 0.0314],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 1., 0., 0., 0., 1.])\n",
      "Batch loss: 3.0139\n",
      "tensor([0.0000, 0.0675, 0.0000, 0.0367, 0.0365, 0.0332],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 0., 1., 1., 0., 0.])\n",
      "Batch loss: 2.9943\n",
      "tensor([0.0000, 0.0676, 0.0000, 0.0370, 0.0354, 0.0346],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 0., 1., 0., 1.])\n",
      "Batch loss: 2.9425\n",
      "tensor([0.0000, 0.0678, 0.0000, 0.0379, 0.0345, 0.0359],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 0., 1., 1., 0., 0.])\n",
      "Batch loss: 2.9157\n",
      "tensor([0.0000, 0.0680, 0.0000, 0.0386, 0.0347, 0.0367],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 1., 1., 0., 0.])\n",
      "Batch loss: 2.9366\n",
      "tensor([0.0000, 0.0686, 0.0000, 0.0383, 0.0348, 0.0364],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 0., 1., 1., 0., 0.])\n",
      "Batch loss: 3.0496\n",
      "tensor([0.0000, 0.0688, 0.0000, 0.0381, 0.0345, 0.0361],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 0., 1., 0., 1.])\n",
      "Batch loss: 2.8938\n",
      "tensor([0.0000, 0.0690, 0.0000, 0.0377, 0.0346, 0.0358],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 1., 0., 0., 0., 1.])\n",
      "Batch loss: 2.8119\n",
      "tensor([0.0000, 0.0692, 0.0000, 0.0381, 0.0346, 0.0364],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 0., 1., 1., 0.])\n",
      "Batch loss: 3.0608\n",
      "tensor([0.0000, 0.0695, 0.0000, 0.0384, 0.0348, 0.0367],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 0., 1., 1., 1., 0.])\n",
      "Batch loss: 3.0521\n",
      "tensor([0.0000, 0.0697, 0.0000, 0.0387, 0.0348, 0.0373],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 1., 0., 1., 0.])\n",
      "Batch loss: 3.1393\n",
      "tensor([0.0000, 0.0701, 0.0000, 0.0377, 0.0348, 0.0373],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 1., 1., 0., 0.])\n",
      "Batch loss: 3.0832\n",
      "tensor([0.0000, 0.0707, 0.0000, 0.0367, 0.0347, 0.0369],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 1., 0., 1., 0.])\n",
      "Batch loss: 2.9887\n",
      "tensor([0.0000, 0.0710, 0.0000, 0.0356, 0.0343, 0.0368],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 0., 1., 1., 0.])\n",
      "Batch loss: 2.8902\n",
      "tensor([0.0000, 0.0711, 0.0000, 0.0349, 0.0342, 0.0371],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 1., 0., 1., 1., 0.])\n",
      "Batch loss: 2.9670\n",
      "tensor([0.0000, 0.0714, 0.0000, 0.0338, 0.0341, 0.0376],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 0., 1., 1., 1., 0.])\n",
      "Batch loss: 3.0428\n",
      "tensor([0.0000, 0.0717, 0.0000, 0.0323, 0.0346, 0.0376],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 0., 1., 0., 0., 1.])\n",
      "Batch loss: 2.9767\n",
      "tensor([0.0000, 0.0718, 0.0000, 0.0308, 0.0351, 0.0376],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([1., 0., 1., 0., 0., 1.])\n",
      "Batch loss: 2.8578\n",
      "tensor([0.0000, 0.0716, 0.0000, 0.0293, 0.0356, 0.0379],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0., 0., 1., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=6, in_channels=32, out_channels=64):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(2, 126), padding=(1, 64))\n",
    "        self.conv2 = nn.Conv2d(out_channels, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
    "\n",
    "        # Assuming input size is (batch_size, 32, H, W)\n",
    "    \n",
    "        #flatten\n",
    "        self.fc1 = nn.Linear(8192, 512)\n",
    "        self.fc2 = nn.Linear(512, in_channels)\n",
    "        self.fc3 = nn.Linear(in_channels, num_classes)\n",
    "\n",
    "    def _get_feature_size(self, shape):\n",
    "        x = torch.zeros(shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        return int(np.prod(x.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(\"Shape after conv1:\", x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        #print(\"Shape after maxpool1:\", x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(\"Shape after conv2:\", x.shape)\n",
    "        x = self.maxpool(x)\n",
    "        #print(\"Shape after maxpool2:\", x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(\"Shape after flattening again:\", x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"Shape after fc1:\", x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "    \n",
    "        return x\n",
    "\n",
    "# Initialize model, criterion, optimizer, and other components\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "#criterion = criterion_function()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device, dtype=torch.float32), y.to(device, dtype=torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        print(y_pred[0])\n",
    "        print(y[0])\n",
    "        loss = criterion_function(y_pred, y)/len(y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X.size(0)  # Accumulate loss\n",
    "        print(f'Batch loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {epoch_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
